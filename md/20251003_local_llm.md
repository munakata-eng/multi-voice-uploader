# セキュリティ重視のAI「ローカルLLM」

こんにちは、むなかたです。

今日は、手元のマシン上でAIを動かせる
「ローカルLLM」について紹介します。

同じ内容は音声でも配信しているので、
ぜひお好きな方法でインプットしてくださいね！

## ローカルLLMとは

普段使っているChatGPTやClaudeなどは
すべての処理はクラウド上で行われるため、
質問した内容が外部に送られるという
セキュリティ上の懸念点がありました。

ローカルLLMでは、自分の手元のマシン上に
AIのモデルをインストールして動かす形式で

・インターネット接続がなくても動く
・機密データを外に出さない
・APIの従量課金が発生しない

というメリットがあります。

## ローカルLLMを導入するハードル

ただし、良いことばかりではありません。
あの優秀なAIたちを動かすためには、
ハイスペックなコンピュータが必要です。

- ストレージ: 数GB〜数十GB規模
- GPU: 大きいVRAMと高い演算性能
- CPU/メモリ: 前処理や小規模推論で使用

実用的なモデルを動かすには、
ふだん使っている一般的なPCでは厳しく
一定以上の性能が要求されます。

## ローカルLLMのできないこと

セキュリティの観点で言うと
外部への検索機能をOFFにせざるを得ないので
「探す」ことには基本的に弱いです。

- 最新情報に基づいた返答
- Web検索を前提とした調査
は難しいでしょう。

使い方がある程度限られてしまうので
どういった場面で使うかをしっかり考えてから
導入したほうが良いですね。

## どこで使うべき？

現状だと「会社でのAI利用」に役立てる
ケースが一番バランスが良い印象です。

- ある程度の先行投資が可能
- 外部に出せないデータを扱うことが多い
- 社内ルールをAIに学習させて共通化

「与えられた情報を加工・整理」する場面では
大きく役立つので、それを踏まえて社内業務に
活用できるかどうかが重要です。

## まとめ

手元のマシン上でAIを動かすローカルLLMは
「情報守秘」と「オフライン性」が強みですが、
個人利用では導入ハードルが高めです。

現状は社内業務用に会社が導入するくらいが
現実的かなという結論になりますね。

今回は以上です。
また次回もよろしくお願いします。
